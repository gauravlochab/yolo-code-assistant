{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Indexing Exploration\n",
    "\n",
    "This notebook explores the indexing pipeline for the YOLO codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.yolo_assistant.config import config\n",
    "from src.yolo_assistant.indexer import TreeSitterParser, CodeChunker, CodeEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Tree-sitter Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test Python file\n",
    "test_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class YOLOModel(nn.Module):\n",
    "    \"\"\"YOLO model base class.\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg='yolov8n.yaml'):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        return self.model(x)\n",
    "        \n",
    "def train_model(model, dataloader, epochs=100):\n",
    "    \"\"\"Train a YOLO model.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        pass\n",
    "'''\n",
    "\n",
    "# Save test file\n",
    "test_file = Path('test_yolo.py')\n",
    "test_file.write_text(test_code)\n",
    "\n",
    "# Parse the file\n",
    "parser = TreeSitterParser()\n",
    "elements = parser.parse_file(test_file)\n",
    "\n",
    "# Display results\n",
    "for element in elements:\n",
    "    print(f\"Type: {element.element_type}\")\n",
    "    print(f\"Name: {element.name}\")\n",
    "    print(f\"Lines: {element.start_line}-{element.end_line}\")\n",
    "    if element.docstring:\n",
    "        print(f\"Docstring: {element.docstring}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Cleanup\n",
    "test_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Code Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chunking a directory\n",
    "chunker = CodeChunker()\n",
    "\n",
    "# Get a sample from the actual repo (if cloned)\n",
    "repo_path = config.ultralytics_repo_dir\n",
    "if repo_path.exists():\n",
    "    # Find a sample Python file\n",
    "    sample_files = list((repo_path / \"ultralytics/models\").glob(\"*.py\"))[:3]\n",
    "    \n",
    "    for file_path in sample_files:\n",
    "        print(f\"\\nChunking {file_path.name}:\")\n",
    "        chunks = chunker.chunk_file(file_path)\n",
    "        \n",
    "        for chunk in chunks[:3]:  # Show first 3 chunks\n",
    "            print(f\"  - {chunk.chunk_type}: {chunk.name}\")\n",
    "            print(f\"    Lines: {chunk.start_line}-{chunk.end_line}\")\n",
    "else:\n",
    "    print(\"Repository not cloned yet. Run 'python main.py --index' first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding generation\n",
    "embedder = CodeEmbedder()\n",
    "\n",
    "# Test single embedding\n",
    "test_text = \"def train_yolo_model(model, dataset, epochs=100):\"\n",
    "embedding = embedder.embed_text(test_text)\n",
    "\n",
    "print(f\"Embedding model: {embedder.model_name}\")\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")\n",
    "\n",
    "# Test batch embedding\n",
    "test_texts = [\n",
    "    \"class YOLO(nn.Module):\",\n",
    "    \"def forward(self, x):\",\n",
    "    \"def loss(self, pred, target):\"\n",
    "]\n",
    "\n",
    "batch_embeddings = embedder.embed_batch(test_texts)\n",
    "print(f\"\\nBatch embeddings shape: {len(batch_embeddings)} x {len(batch_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Full Indexing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the full pipeline on a small directory\n",
    "if repo_path.exists():\n",
    "    # Create test chunks\n",
    "    test_dir = repo_path / \"ultralytics/models/yolo\"\n",
    "    if test_dir.exists():\n",
    "        chunks = chunker.chunk_directory(test_dir, recursive=False)\n",
    "        print(f\"Found {len(chunks)} chunks\")\n",
    "        \n",
    "        # Prepare for storage\n",
    "        documents = embedder.prepare_chunks_for_storage(chunks[:5])  # Just first 5\n",
    "        \n",
    "        # Check document structure\n",
    "        if documents:\n",
    "            print(\"\\nDocument structure:\")\n",
    "            for key in documents[0].keys():\n",
    "                if key != 'embedding':\n",
    "                    print(f\"  - {key}: {type(documents[0][key])}\")\n",
    "            print(f\"  - embedding: list[float] (dim={len(documents[0]['embedding'])})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
