{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Retrieval Testing\n",
    "\n",
    "This notebook tests the retrieval accuracy of the YOLO Code Assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.yolo_assistant.config import config\n",
    "from src.yolo_assistant.storage import MongoDBVectorStore\n",
    "from src.yolo_assistant.retrieval import CodeSearcher, ResultRanker\n",
    "from src.yolo_assistant.indexer import CodeEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to MongoDB and Check Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "vector_store = MongoDBVectorStore()\n",
    "embedder = CodeEmbedder()\n",
    "searcher = CodeSearcher(vector_store, embedder)\n",
    "ranker = ResultRanker()\n",
    "\n",
    "# Connect and check status\n",
    "try:\n",
    "    vector_store.connect()\n",
    "    stats = vector_store.get_statistics()\n",
    "    \n",
    "    print(\"MongoDB Vector Store Statistics:\")\n",
    "    print(f\"  Total chunks: {stats['total_chunks']}\")\n",
    "    print(f\"  Files indexed: {stats['files_indexed']}\")\n",
    "    print(f\"  Index status: {stats['index_status']}\")\n",
    "    print(\"\\nChunks by type:\")\n",
    "    for chunk_type, count in stats['chunks_by_type'].items():\n",
    "        print(f\"    {chunk_type}: {count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to MongoDB: {e}\")\n",
    "    print(\"Make sure you have:\")\n",
    "    print(\"  1. Set MONGODB_URI in .env\")\n",
    "    print(\"  2. Run 'python main.py --index' to index the codebase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Basic Search Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"How to train a YOLO model?\",\n",
    "    \"YOLO model architecture\",\n",
    "    \"Data augmentation in YOLO\",\n",
    "    \"Export model to ONNX\",\n",
    "    \"Calculate mAP metric\"\n",
    "]\n",
    "\n",
    "# Perform searches\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Search\n",
    "    results = searcher.search(query, limit=3)\n",
    "    \n",
    "    # Rank results\n",
    "    ranked_results = ranker.rank_results(results, query)\n",
    "    \n",
    "    # Enhance with summaries\n",
    "    enhanced_results = ranker.enhance_results_with_summary(ranked_results)\n",
    "    \n",
    "    # Display results\n",
    "    if enhanced_results:\n",
    "        for i, result in enumerate(enhanced_results, 1):\n",
    "            print(f\"\\n[{i}] {result.get('summary', 'No summary')}\")\n",
    "            print(f\"    Score: {result.get('search_score', 0):.4f}\")\n",
    "            if result.get('docstring'):\n",
    "                print(f\"    Docstring: {result['docstring'][:100]}...\")\n",
    "    else:\n",
    "        print(\"  No results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Specific Search Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function search\n",
    "print(\"Function Search Test:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "function_results = searcher.search_by_function_name(\"train\", limit=5)\n",
    "print(f\"Found {len(function_results)} functions with 'train' in name:\")\n",
    "for result in function_results:\n",
    "    print(f\"  - {result['name']} ({result['file_path'].split('/')[-1]}:{result['start_line']})\")\n",
    "\n",
    "# Test class search\n",
    "print(\"\\nClass Search Test:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "class_results = searcher.search_by_class_name(\"YOLO\", include_methods=True)\n",
    "print(f\"Found {len(class_results)} classes/methods with 'YOLO' in name:\")\n",
    "\n",
    "# Group by class\n",
    "classes = {}\n",
    "for result in class_results:\n",
    "    if result['chunk_type'] == 'class':\n",
    "        classes[result['name']] = {'methods': []}\n",
    "    elif result['chunk_type'] == 'method' and result.get('parent_class'):\n",
    "        if result['parent_class'] not in classes:\n",
    "            classes[result['parent_class']] = {'methods': []}\n",
    "        classes[result['parent_class']]['methods'].append(result['name'])\n",
    "\n",
    "for class_name, info in classes.items():\n",
    "    print(f\"  Class: {class_name}\")\n",
    "    for method in info['methods'][:5]:  # Show first 5 methods\n",
    "        print(f\"    - {method}()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Retrieval Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Performance test queries\n",
    "perf_queries = [\n",
    "    \"train model\",\n",
    "    \"data loader\",\n",
    "    \"loss function\",\n",
    "    \"model export\",\n",
    "    \"validation metrics\"\n",
    "]\n",
    "\n",
    "# Measure performance\n",
    "performance_data = []\n",
    "\n",
    "for query in perf_queries:\n",
    "    # Vector search\n",
    "    start_time = time.time()\n",
    "    vector_results = searcher.search(query, use_embeddings=True)\n",
    "    vector_time = time.time() - start_time\n",
    "    \n",
    "    # Text search\n",
    "    start_time = time.time()\n",
    "    text_results = searcher.search(query, use_embeddings=False)\n",
    "    text_time = time.time() - start_time\n",
    "    \n",
    "    performance_data.append({\n",
    "        'Query': query,\n",
    "        'Vector Search Time (ms)': vector_time * 1000,\n",
    "        'Vector Results': len(vector_results),\n",
    "        'Text Search Time (ms)': text_time * 1000,\n",
    "        'Text Results': len(text_results)\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "df = pd.DataFrame(performance_data)\n",
    "print(\"Retrieval Performance Comparison:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Average vector search time: {df['Vector Search Time (ms)'].mean():.2f} ms\")\n",
    "print(f\"Average text search time: {df['Text Search Time (ms)'].mean():.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Result Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ranking effectiveness\n",
    "test_query = \"How to train a YOLO model with custom dataset?\"\n",
    "\n",
    "# Get search results\n",
    "raw_results = searcher.search(test_query, limit=10)\n",
    "\n",
    "# Apply ranking\n",
    "ranked_results = ranker.rank_results(raw_results, test_query)\n",
    "\n",
    "# Compare before and after ranking\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"\\nTop 5 results before ranking:\")\n",
    "for i, result in enumerate(raw_results[:5], 1):\n",
    "    print(f\"{i}. {result['name']} (score: {result.get('search_score', 0):.4f})\")\n",
    "\n",
    "print(\"\\nTop 5 results after ranking:\")\n",
    "for i, result in enumerate(ranked_results[:5], 1):\n",
    "    combined_score = (\n",
    "        result.get('search_score', 0) * 0.7 +\n",
    "        result.get('relevance_score', 0) * 0.3\n",
    "    )\n",
    "    print(f\"{i}. {result['name']} (combined score: {combined_score:.4f})\")\n",
    "    print(f\"   Vector: {result.get('search_score', 0):.4f}, Relevance: {result.get('relevance_score', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Context Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieving with context\n",
    "context_query = \"model validation\"\n",
    "\n",
    "# Search with context\n",
    "results_with_context = searcher.search_with_context(context_query, context_size=2)\n",
    "\n",
    "# Display results with context\n",
    "print(f\"Query: {context_query}\")\n",
    "print(f\"Found {len(results_with_context)} results\\n\")\n",
    "\n",
    "for i, result in enumerate(results_with_context[:2], 1):\n",
    "    print(f\"Result {i}: {result['name']}\")\n",
    "    print(f\"  File: {result['file_path']}\")\n",
    "    print(f\"  Lines: {result['start_line']}-{result['end_line']}\")\n",
    "    \n",
    "    if 'context_chunks' in result:\n",
    "        print(f\"  Context ({len(result['context_chunks'])} surrounding chunks):\")\n",
    "        for ctx in result['context_chunks']:\n",
    "            print(f\"    - {ctx['chunk_type']} {ctx['name']} (lines {ctx['start_line']}-{ctx['end_line']})\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
